You can use the Airflow to extract the Api 

Create a directory named assignment
detest@UbuntuDE:~$ mkdir assignment
Data extraction from api through Nifi

Processors that used:
InvokeHTTP
SplitJson
UpdateAttribute
PutFile

Configuration:
InvokeHTTP Processor
In Properties,
HTTP Method: GET
Remote URL: <API URL>
In Settings,
Automatically terminate relationships(Check the following options)
☑️ Failure
☑️ No Retry
☑️ Original
☑️ Retry
SplitJson Processor
In Properties,
JsonPath Expression: $.*
In Settings,
Automatically terminate relationships(Check the following options)
☑️ Failure
☑️ Original
UpdateAttribute Processor
In Properties,
Add property filename: StockData.json
PutFile Processor
Directory: /home/detest/assignment
Conflict Resolution Strategy: ignore
Create Missing Directories: true
In Settings,
Automatically terminate relationships(Check the following options)
☑️ Failure
☑️ Original

Connections and their configurations:
InvokeHTTP >> SplitJson Connection
In Details,
For Relationships (Check the following option)
☑️ Response
In Settings,
Set object threshold=1
SplitJson >> UpdateAttribute Connection
In Details,
For Relationships (Check the following option)
☑️ Split
In Settings,
Set object threshold=2
UpdateAttribute >> PutFile Connection
In Details,
For Relationships (Check the following option)
☑️ Success
In Settings,
Set object threshold=2

After running the flow, a StockData.json file was created.

Create a dag named stock_analysis.py

import datetime as dt
from datetime import timedelta
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.ar_model import AutoReg
from matplotlib import pyplot
from sklearn.metrics import mean_squared_error
from math import sqrt
import csv
from matplotlib import pyplot as plt
import seaborn as sns

default_args = {
	'owner': 'detest',
	'start_date': dt.datetime(2024, 9, 5),
	'retries': 1,
	'retry_delay': dt.timedelta(minutes=5),
}

def extract():
    	df=pd.read_json('/home/detest/assignment/StockData.json')
    	df1=df.transpose()
    	df2=df1.reset_index()
    	df3=df2.sort_values(by='index',ascending=True)
    	df3.rename(columns={'index': 'date'}, inplace=True)
    	df4=df3.set_index('date')
    	data = df4.filter(['4. close'])
    	data.to_csv('/home/detest/assignment/data.csv')

def corrmap():
    	df=pd.read_json('/home/detest/assignment/StockData.json')
    	df1=df.transpose()
    	df2=df1.reset_index()
    	df3=df2.sort_values(by='index',ascending=True)
    	df3.rename(columns={'index': 'date'}, inplace=True)
    	df4=df3.set_index('date')
    	data=pd.DataFrame(df4[['1. open','2. high','3. low','4. close','5. adjusted close','6.         volume','7. dividend amount','8. split coefficient']])
    	returns=data.pct_change()
    	correlation_matrix=returns.corr()
    	plt.figure(figsize=(10, 8))
    	sns.heatmap(correlation_matrix, annot=True, cmap='viridis', linewidths=.5, linecolor='black')
    	plt.title('Correlation Matrix of Daily Returns')
    	output_image_file = '/home/detest/assignment/correlation.png'
    	plt.savefig(output_image_file, format='png')

def training():
    	series=pd.read_csv('/home/detest/assignment/data.csv',parse_dates=True)
    	series=series.squeeze()
    	values=pd.DataFrame(series['4. close'].values)
    	dataframe = pd.concat([values.shift(1), values], axis=1)
    	dataframe.columns = ['t', 't+1']
    	X = dataframe.values
    	train_size = int(len(X) * 0.66)
    	train, test = X[1:train_size], X[train_size:]
    	train_X, train_y = train[:,0], train[:,1]
    	test_X, test_y = test[:,0], test[:,1]
    	train_pred = [x for x in train_X]
    	train_resid = [train_y[i]-train_pred[i] for i in range(len(train_pred))]
    	model = AutoReg(train_resid,20)
    	model_fit = model.fit()
    	window = len(model_fit.ar_lags)
    	coef = model_fit.params
    	history = train_resid[len(train_resid)-window:]
    	history = [history[i] for i in range(len(history))]
    	predictions = list()
    	for t in range(len(test_y)):
            	yhat = test_X[t]
            	error = test_y[t] - yhat
            	length = len(history)
            	lag = [history[i] for i in range(length-window,length)]
            	pred_error = coef[0]
            	for d in range(window):
                    	pred_error += coef[d+1] * lag[window-d-1]
            	yhat = yhat + pred_error
            	predictions.append(yhat)
            	history.append(error)
    	results = list(zip(test_y, predictions))
    	output_file = '/home/detest/assignment/prediction.csv'
    	with open(output_file, 'w', newline='') as file:
            	writer = csv.writer(file)
            	writer.writerow(["Expected", "Predicted"])
            	writer.writerows(results)

def visual():
    	data=pd.read_csv('/home/detest/assignment/prediction.csv')
    	plt.figure(figsize=(10, 6))
    	plt.plot(data['Expected'], label='Expected')
    	plt.plot(data['Predicted'], label='Predicted', color='yellow')
    	plt.xlabel('Time Steps')
    	plt.ylabel('Stock_price')
    	plt.title('Predicted vs Expected')
    	plt.legend()
    	output_image_file = '/home/detest/assignment/ExpVSPred.png'
    	plt.savefig(output_image_file, format='png')

with DAG('StockDataAnalysis',
	default_args=default_args,
	schedule_interval='@once',
) as dag:
    	Extract = PythonOperator(task_id='Extracting_Data',python_callable=extract)
    	CorrPlot=PythonOperator(task_id='Plotting_Correlation_Plot',python_callable=corrmap)
    	Training=PythonOperator(task_id='Training_Predicting_Data',python_callable=training)
    	Visualize=PythonOperator(task_id='Plotting_ExpVSPred_Plot',python_callable=visual)
Extract >> CorrPlot >> Training >> Visualize

Start the airflow webserver and scheduler

detest@UbuntuDE:~$ airflow webserver
detest@UbuntuDE:~$ airflow scheduler

In this Dag the Autoregressive model used for predicting the stock prices
After Running Dag following charts are formed.

The following chart shows a comparison between the Predicted and Expected stock prices. 

After taking glance of this chart the model is perfectly fit and you can check the predicted values also (the model stores expected and predicted values in the prediction.csv file).

You can check the accuracy of model
  
Here the error is too small compared to the size of the data, So the model is perfectly fit.

you can use another prediction algorithms also.
